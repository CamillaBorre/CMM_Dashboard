{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa92eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file, save\n",
    "\n",
    "#from bokeh.embed import components, file_html\n",
    "from bokeh.io import output_notebook, show, curdoc\n",
    "from bokeh.models import Panel, Tabs, Slider, CustomJS, HoverTool, ColumnDataSource, Range1d, GraphRenderer,Text,Legend\n",
    "from bokeh.models import SingleIntervalTicker, LinearAxis, TableColumn, DataTable, HTMLTemplateFormatter\n",
    "from bokeh.models.widgets import RangeSlider, Select, RadioGroup, MultiSelect, TextInput\n",
    "from bokeh.layouts import widgetbox, gridplot\n",
    "#from bokeh.resources import CDN\n",
    "from bokeh.layouts import column, row, layout\n",
    "from bokeh.palettes import inferno, viridis, Turbo256 # select a palette\n",
    "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, ColumnDataSource, PrintfTickFormatter, Whisker\n",
    "from bokeh.transform import transform\n",
    "from bokeh.models.mappers import LogColorMapper\n",
    "from bokeh.models import Title\n",
    "\n",
    "from scipy.stats import shapiro \n",
    "\n",
    "from bokeh.colors import RGB\n",
    "from matplotlib import cm\n",
    "\n",
    "#from bokeh.palettes import Dark2_5 as palette\n",
    "import itertools  \n",
    "#output_notebook()\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "import sys\n",
    "\n",
    "from datetime import datetime as datetime_import\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import anderson\n",
    "\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "Version = 'v2.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_calculation(df_full):\n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2531f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cube_calculations(df_full):\n",
    "    #Calculations\n",
    "\n",
    "    #Scaling\n",
    "    X_target = ['TF','TB','DF','DB']\n",
    "    Y_target = ['TL','TR','DL','DR']\n",
    "    Z_target = ['FL','FR','BL','BR']\n",
    "    df_full['X_scaling'] = 0\n",
    "    df_full['Y_scaling'] = 0\n",
    "    df_full['Z_scaling'] = 0\n",
    "\n",
    "    for i in range(len(df_full)):\n",
    "        X_scaling = np.array([])\n",
    "        Y_scaling = np.array([])\n",
    "        Z_scaling = np.array([])\n",
    "        for T in range(len(X_target)):\n",
    "\n",
    "            df_cube = df_full.loc[i]\n",
    "            X_scaling = np.append(X_scaling, np.append( ( \n",
    "                df_cube[X_target[T]+'A']+df_cube[X_target[T]+'B'])/20, (df_cube[X_target[T]+'C']+df_cube[X_target[T]+'B'])/20))\n",
    "            Y_scaling = np.append(Y_scaling, np.append( ( \n",
    "                df_cube[Y_target[T]+'A']+df_cube[Y_target[T]+'B'])/20, (df_cube[Y_target[T]+'C']+df_cube[Y_target[T]+'B'])/20))\n",
    "            Z_scaling = np.append(Z_scaling, np.append( ( \n",
    "                df_cube[Z_target[T]+'A']+df_cube[Z_target[T]+'B'])/20, (df_cube[Z_target[T]+'C']+df_cube[Z_target[T]+'B'])/20))\n",
    "        df_full.loc[i, 'X_scaling'] = np.mean((X_scaling-1))*100\n",
    "        df_full.loc[i, 'Y_scaling'] = np.mean((Y_scaling-1))*100\n",
    "        df_full.loc[i, 'Z_scaling'] = np.mean((Z_scaling-1))*100\n",
    "\n",
    "    #FaceOffset \n",
    "    F_target = ['TF','TB','TL','TR','DF','DB','DL','DR']\n",
    "    df_full['FaceOffest'] = 0\n",
    "    for i in range(len(df_full)):\n",
    "        FO = np.array([])\n",
    "        df_cube = df_full.loc[i]\n",
    "        for T in range(len(F_target)):\n",
    "            FO = np.append(FO, np.append( ( \n",
    "                df_cube[F_target[T]+'A']-df_cube[F_target[T]+'B'])/4, (df_cube[F_target[T]+'C']-df_cube[F_target[T]+'B'])/4))\n",
    "        df_full.loc[i, 'FaceOffest'] = -np.mean(FO)\n",
    "\n",
    "\n",
    "    #ZOverCuring\n",
    "    ZOC_target = ['FL','FR','BL','BR']\n",
    "    df_full['ZOverCuring'] = 0\n",
    "    for i in range(len(df_full)):\n",
    "        ZOC = np.array([])\n",
    "        df_cube = df_full.loc[i]\n",
    "        for T in range(len(ZOC_target)):\n",
    "            ZOC = np.append(ZOC, np.mean(np.append( ( \n",
    "                df_cube[ZOC_target[T]+'A']-df_cube[ZOC_target[T]+'B'])/4, (df_cube[ZOC_target[T]+'C']-df_cube[ZOC_target[T]+'B'])/4)))\n",
    "        df_full.loc[i, 'ZOverCuring'] = -np.mean(ZOC*2)\n",
    "\n",
    "\n",
    "    #Accuracy \n",
    "    F_target = ['TF','TB','TL','TR','DF','DB','DL','DR']\n",
    "    df_full['Accuracy_XY'] = 0\n",
    "    for i in range(len(df_full)):\n",
    "        Accuracy_array = np.array([])\n",
    "        for T in range(len(F_target)):\n",
    "            df_cube = df_full.loc[i]\n",
    "\n",
    "            Accuracy_array = np.append(Accuracy_array,  [df_cube[F_target[T]+'A'],df_cube[F_target[T]+'B'],df_cube[F_target[T]+'C']] )\n",
    "        Deviation = np.mean(np.mean(Accuracy_array-10))\n",
    "        Std = np.std(Accuracy_array, ddof=1) #sample std. Without ddof=1 population std\n",
    "        Accuracy = abs(Deviation) + 2*Std \n",
    "        df_full.loc[i, 'Accuracy_XY'] = np.mean(Accuracy)\n",
    "\n",
    "    #Accuracy \n",
    "    F_target = ['FL','FR','BL','BR','TF','TB','TL','TR','DF','DB','DL','DR']\n",
    "    df_full['Accuracy'] = 0\n",
    "    for i in range(len(df_full)):\n",
    "        Accuracy_array = np.array([])\n",
    "        for T in range(len(F_target)):\n",
    "            df_cube = df_full.loc[i]\n",
    "\n",
    "            Accuracy_array = np.append(Accuracy_array,  [df_cube[F_target[T]+'A'],df_cube[F_target[T]+'B'],df_cube[F_target[T]+'C']] )\n",
    "        Deviation = np.mean(np.mean(Accuracy_array-10))\n",
    "        Std = np.std(Accuracy_array, ddof=1) #sample std. Without ddof=1 population std\n",
    "        Accuracy = abs(Deviation) + 2*Std \n",
    "        df_full.loc[i, 'Accuracy'] = np.mean(Accuracy)\n",
    "\n",
    "    # Shifting\n",
    "    df_full['ShiftingXZ'] = 0\n",
    "    df_full['ShiftingYZ'] = 0\n",
    "    df_full['ShiftingXY'] = 0\n",
    "    for i in range(len(df_full)):\n",
    "        df_cube = df_full.loc[i]\n",
    "        ShiftingXZ = np.mean((90-df_cube['Angle DL'] + df_cube['Angle DR']-90 + df_cube['Angle TL']-90 + 90-df_cube['Angle TR'])/4)\n",
    "        ShiftingYZ = np.mean((df_cube['Angle TF']-90 + 90-df_cube['Angle TB'] + 90-df_cube['Angle DF'] + df_cube['Angle DB']-90)/4)\n",
    "        ShiftingXY = np.mean((df_cube['Angle BL']-90 + 90-df_cube['Angle FL'] + 90-df_cube['Angle BR'] + df_cube['Angle FR']-90)/4)\n",
    "        df_full.loc[i, 'ShiftingXZ'] = ShiftingXZ\n",
    "        df_full.loc[i, 'ShiftingYZ'] = ShiftingYZ\n",
    "        df_full.loc[i, 'ShiftingXY'] = ShiftingXY\n",
    "        \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HoReRe_calculation(df_full):\n",
    "    \n",
    "    thickness_CAD = 8\n",
    "    width_CAD = 8\n",
    "    Knob_CAD = 5\n",
    "    \n",
    "    #Target parameters\n",
    "    #Shifting\n",
    "    df_full['XZ_Shear'] = np.mean( [-(df_full['Angle_D_F']-90) + (df_full['Angle_C_D']-90) - (df_full['Angle_A_C']-90) +(df_full['Angle_A_F']-90)],axis=0 )\n",
    "    df_full['YZ_Shear'] = np.mean( [-(df_full['Angle_E_F']-90) + (df_full['Angle_B_F']-90) - (df_full['Angle_B_C']-90) +(df_full['Angle_C_E']-90)],axis=0 )\n",
    "    df_full['XY_Shear'] = np.mean( [-(df_full['Angle_D_E']-90) + (df_full['Angle_A_E']-90) - (df_full['Angle_A_B']-90) +(df_full['Angle_B_D']-90)],axis=0 )\n",
    "\n",
    "\n",
    "    #Calculate Face_offset (Only for HoReRe cubes)\n",
    "    df_full['Face_offset_X'] = (np.mean([df_full['BFU34']-thickness_CAD,df_full['BFU12']-thickness_CAD],axis=0) - (df_full['BFU23']-width_CAD)  )/4\n",
    "    df_full['Face_offset_Y'] = (np.mean([df_full['AFV12']-thickness_CAD,df_full['AFV34']-thickness_CAD],axis=0) - (df_full['AFV23']-width_CAD)  )/4\n",
    "    df_full['Face_offset_Z'] = (np.mean([df_full['AEW12']-thickness_CAD,df_full['AEW34']-thickness_CAD],axis=0) - (df_full['AEW23']-width_CAD)  )/4\n",
    "\n",
    "    df_full['Face_offset_XY'] = np.mean([df_full['Face_offset_X'],df_full['Face_offset_Y']],axis=0)\n",
    "\n",
    "    df_full['Face_offset_XY_diameter'] = (df_full['ABFW_diameter']-Knob_CAD)/2\n",
    "\n",
    "\n",
    "    #Calculate Scaling (Only for HoReRe cubes)\n",
    "    df_full['Scaling_X_pct'] = (np.mean([ df_full['BFU12']+df_full['BFU23'], df_full['BFU23']+df_full['BFU34']],axis=0) / (thickness_CAD+width_CAD)-1)*100\n",
    "    df_full['Scaling_Y_pct'] = (np.mean([ df_full['AFV12']+df_full['AFV23'], df_full['AFV23']+df_full['AFV34']],axis=0) / (thickness_CAD+width_CAD)-1)*100\n",
    "    df_full['Scaling_Z_pct'] = (np.mean([ df_full['AEW12']+df_full['AEW23'], df_full['AEW23']+df_full['AEW34']],axis=0) / (thickness_CAD+width_CAD)-1)*100\n",
    "\n",
    "\n",
    "    df_full['Scaling_X_noW_pct'] = (np.mean([ df_full['BFU12'], df_full['BFU34']],axis=0) / (thickness_CAD)-1)*100\n",
    "    df_full['Scaling_Y_noW_pct'] = (np.mean([ df_full['AFV12'], df_full['AFV34']],axis=0) / (thickness_CAD)-1)*100\n",
    "    df_full['Scaling_Z_noW_pct'] = (np.mean([ df_full['AEW12'], df_full['AEW34']],axis=0) / (thickness_CAD)-1)*100\n",
    "\n",
    "\n",
    "    df_full['Z_compensation'] = df_full['Face_offset_Z']*2\n",
    "\n",
    "    df_full['Z_compensation_diameter'] = abs(np.mean([df_full['BCDU_diameter'],df_full['ACEV_diameter']],axis=0) - df_full['ABFW_diameter'])\n",
    "    df_full['Z_compensation_roundness'] = abs(np.mean([df_full['BCDU_Roundness'],df_full['ACEV_Roundness']],axis=0) - df_full['ABFW_Roundness'])\n",
    "\n",
    "   \n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Carl_caluclation(df_full):\n",
    "    #Target parameters\n",
    "    M500A_CAD = 3\n",
    "    \n",
    "    \n",
    "    df_full['Scaling_X_pct'] = (df_full['M500A']/M500A_CAD - 1) * 100\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780c79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_file(Path,Data_folder,Save_folder,Version,PartID_loc):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    folder = Data_folder\n",
    "\n",
    "    if os.path.exists(Save_folder)==False:\n",
    "        os.mkdir(Save_folder) \n",
    "\n",
    "\n",
    "    df_full = pd.DataFrame({})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for Folder in os.listdir(folder):\n",
    "        if Folder[0]!='.':\n",
    "            date = Folder.split(' ')[-1]\n",
    "            date = datetime_import.strptime(date,'%d%m%Y')\n",
    "            machine = Folder.split(' ')[-2]\n",
    "            job = Folder.split(' ')[-3]\n",
    "            df_all = pd.DataFrame({})\n",
    "\n",
    "\n",
    "\n",
    "            for file in os.listdir(folder+Folder):\n",
    "\n",
    "                if file.endswith('xls') | file.endswith('xlsx'):\n",
    "                    df = pd.read_excel(folder+Folder+'/'+file,skiprows=12)\n",
    "                    name = file.split('.')[0].split('_')[-1]\n",
    "                    df = df.rename(columns = {'Characteristic':'index'})\n",
    "\n",
    "                    df_flip = pd.DataFrame([np.asarray(df['Actual'])],columns=df['index'])\n",
    "                    df_flip['Date'] = date\n",
    "                    df_flip['Machine'] = machine\n",
    "                    df_flip.insert(0, 'Machine', df_flip.pop('Machine'))\n",
    "                    df_flip['Print_ID'] = job\n",
    "                    df_flip.insert(0, 'Print_ID', df_flip.pop('Print_ID'))\n",
    "                    df_flip['Cube'] = name\n",
    "                    df_flip.insert(0, 'Cube', df_flip.pop('Cube'))    \n",
    "                    \n",
    "                    \n",
    "                    part = (int(name[-1])-1)*9 + ( ord(name[0]) - 64 )\n",
    "                    \n",
    "                    df_flip.insert(0, 'Part_number', part ) \n",
    "\n",
    "                    df_all = pd.concat([df_all,df_flip])\n",
    "                    \n",
    "            df_all = df_all.reset_index(inplace = False,drop=True) \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            #df_all = extra_calculation(df_all)\n",
    "\n",
    "            df_full = pd.concat([df_all,df_full])\n",
    "            \n",
    "            \n",
    "            \n",
    "    # Calibration cubes\n",
    "    Calibration_cubes = True\n",
    "    if Calibration_cubes: \n",
    "        Layout_file = 'JB266_Unscaled.openjob'\n",
    "        extra_calculations = extra_calculation\n",
    "        \n",
    "        part_indx = 0\n",
    "    \n",
    "    \n",
    "    Openjob_folder = 'openjob'\n",
    "    with open(Openjob_folder+'/'+Layout_file, 'r') as f:\n",
    "        Layout_data = f.read() \n",
    "\n",
    "    # Passing the stored data inside the beautifulsoup parser \n",
    "    bs_data = BeautifulSoup(Layout_data, 'xml') \n",
    "\n",
    "    # Finding all instances of tag   \n",
    "    b_unique = bs_data.find_all('part') \n",
    "\n",
    "    Layout = pd.DataFrame({})\n",
    "\n",
    "    for i in range(0,len(b_unique)):\n",
    " \n",
    "        part = b_unique[i][\"name\"].split('_')[part_indx]\n",
    "        part = part.split('.')[0]\n",
    "    \n",
    "\n",
    "        xcoor_build = np.float64(b_unique[i].translation.x.string)\n",
    "        ycoor_build = np.float64(b_unique[i].translation.y.string)\n",
    "        zcoor_build = np.float64(b_unique[i].translation.z.string)\n",
    "\n",
    "\n",
    "        Layout = pd.concat([Layout,pd.DataFrame({'Part_number': [np.int64(part)],'X_coord':[xcoor_build],'Y_coord':[ycoor_build],'Z_coord':[zcoor_build]})])\n",
    "    Layout = Layout.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    df_full = df_full.merge(Layout, on = 'Part_number', how = 'inner')  \n",
    "\n",
    "\n",
    "    Layout_layers = pd.DataFrame({'Layer': np.unique(Layout['Z_coord'])})\n",
    "\n",
    "\n",
    "\n",
    "    #----Layout End----\n",
    "    df_full = df_full.reset_index(drop=True)\n",
    "    \n",
    "    df_full = Cube_calculations(df_full)\n",
    "\n",
    "    df_full['Layer'] = 9999    \n",
    "    for layers in range(len(Layout_layers)):\n",
    "        df_full['Layer'][df_full['Z_coord'] == Layout_layers['Layer'][layers]] = layers+1\n",
    "\n",
    "    df_full['Galvo'] = 1  \n",
    "    \n",
    "\n",
    "\n",
    "    df_full['Print_ID_layer'] = df_full['Print_ID'] + '_' + df_full['Layer'].astype(str)\n",
    "\n",
    "    df_full['Print_ID_galvo'] = df_full['Print_ID'] + '_' + df_full['Galvo'].astype(str)\n",
    "    \n",
    "    \n",
    "    df_full['Version'] = Version\n",
    "    \n",
    "    df_full.columns = df_full.columns.str.replace(' ', '_')\n",
    "    \n",
    "    df_full.to_csv(Save_folder+\"/all_combined_values.csv\", index=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return df_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a207ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting of heatmapes\n",
    "def heatmaps(data,parameters,job,Colourmap_range,method,coolwarm_palette,plots,Visual):\n",
    "    \n",
    "    \n",
    "    IDs = data[['Part_number','Print_ID','Print_ID_layer','Print_ID_galvo','Layer','X_coord','Y_coord']] #remember add galvo\n",
    "    \n",
    "    data_dropped = data.drop(columns=['Part_number','Print_ID','Print_ID_layer','Print_ID_galvo','Layer','Version']) #remember add galvo\n",
    "    \n",
    "    #Plot mean, std or result values in heatmap\n",
    "    if method == 'Mean':\n",
    "        \n",
    "        df_sub = data_dropped.groupby(['X_coord','Y_coord']).mean(numeric_only=True).reset_index()\n",
    "\n",
    "        df_sub = df_sub.merge(IDs, on = ['X_coord','Y_coord'])\n",
    "        \n",
    "        df_sub = df_sub[df_sub['Print_ID'] == np.unique(df_sub['Print_ID'])[0]] \n",
    "        \n",
    "    elif method == 'Std':\n",
    "            \n",
    "        df_sub = data_dropped.groupby(['X_coord','Y_coord']).std().reset_index()\n",
    "        \n",
    "        df_sub = df_sub.merge(IDs, on = ['X_coord','Y_coord'])\n",
    "        \n",
    "        df_sub = df_sub[df_sub['Print_ID'] == np.unique(df_sub['Print_ID'])[0]] \n",
    "        \n",
    "        \n",
    "    for parameter in parameters:\n",
    "\n",
    "        #If more layers, split based on layer\n",
    "        if method == 'Original':\n",
    "            if 'Print_ID_layer' in data.columns:\n",
    "                df_sub = data[data['Print_ID_layer']==job]\n",
    "            else:\n",
    "                df_sub = data[data['Print_ID']==job]\n",
    "\n",
    "                \n",
    "        #If only one job table becomes NaN. Replace with 0. \n",
    "        if np.all(np.isnan(df_sub[parameter].values)):\n",
    "            df_sub = df_sub.replace(np.nan, 0)\n",
    "        df_sub[parameter+'_mini'] = df_sub[parameter].round(2)\n",
    "\n",
    "\n",
    "\n",
    "        #Define range for colormap   \n",
    "        if method == 'Std':\n",
    "                lower = df_sub[parameter].min()\n",
    "                upper = df_sub[parameter].max()\n",
    "        else:\n",
    "            if parameter+'_lower' in Colourmap_range:\n",
    "                lower = Colourmap_range[parameter+'_lower'].values[0]\n",
    "                upper = Colourmap_range[parameter+'_upper'].values[0]\n",
    "            else:\n",
    "                lower = data[parameter].min()\n",
    "                upper = data[parameter].max()\n",
    "            \n",
    "            \n",
    "\n",
    "        mapper = LinearColorMapper(\n",
    "        palette=coolwarm_palette, low=lower, high=upper)\n",
    "\n",
    "        source = ColumnDataSource(df_sub )\n",
    "        \n",
    "\n",
    "        #Make heatmap plot\n",
    "        pixel_x = np.int64(df_sub[\"Y_coord\"].max()+np.median(np.diff(np.sort(np.unique(df_sub[\"Y_coord\"])))))\n",
    "        pixel_y =  np.int64(df_sub[\"X_coord\"].max()+np.median(np.diff(np.sort(np.unique(df_sub[\"X_coord\"])))))\n",
    "        pixel_factor = 2\n",
    "        \n",
    "        x_range = Range1d(start = 0, end = pixel_y)\n",
    "        y_range = Range1d(start = 0, end = pixel_x)\n",
    "\n",
    "        plot_width = 800\n",
    "        plot_height = 400\n",
    "        \n",
    "        p = figure(\n",
    "            plot_width=plot_width,\n",
    "            plot_height=plot_height,\n",
    "            title=parameter,\n",
    "            #x_axis_type=None,\n",
    "            #y_axis_type=None,\n",
    "            x_range=x_range, \n",
    "            y_range=y_range\n",
    "        )\n",
    "        \n",
    "        p.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'), \"right\")\n",
    "        \n",
    "\n",
    "        \n",
    "        x_offset = np.int64(df_sub[\"X_coord\"].min()/2)\n",
    "        y_offset = np.int64(df_sub[\"Y_coord\"].min()/2)\n",
    "        \n",
    "        \n",
    "        if Visual == 'Geometry' and not os.path.isdir('Geometry'):# (image_file != None): \n",
    "            print('Make folder called Geometry with a png file of your geometry if you want pictures. \\\n",
    "                      Defaulting to regtangles')\n",
    "            \n",
    "            print(len(df_sub)>1)\n",
    "            print(df_sub)\n",
    "            if len(df_sub)>1:\n",
    "                width = np.int64(np.min(np.diff(np.sort(np.unique(df_sub[\"X_coord\"])))))\n",
    "                height = np.int64(np.min(np.diff(np.sort(np.unique(df_sub[\"Y_coord\"])))))\n",
    "            else:\n",
    "                width = 10\n",
    "                height = 10\n",
    "            \n",
    "            p.rect(\n",
    "                x=\"X_coord\",\n",
    "                y=\"Y_coord\",\n",
    "                width=width,\n",
    "                height=height,\n",
    "                source=source,\n",
    "                line_color='black',\n",
    "                line_width = 0.5,\n",
    "                fill_color=transform(parameter, mapper))\n",
    "        elif Visual == 'Geometry' and os.path.isdir('Geometry'):# and (image_file != None):\n",
    "\n",
    "            \n",
    "\n",
    "            image_file = os.listdir('Geometry')[0]\n",
    "            \n",
    "            image = cv2.imread('Geometry/'+ image_file,cv2.IMREAD_GRAYSCALE)\n",
    "            image = np.asarray(image).astype('float').round(2)\n",
    "               \n",
    "            \n",
    "            \n",
    "            im_h = np.int64(np.floor(np.shape(image)[1]/10))\n",
    "            im_w = np.int64(np.floor(np.shape(image)[0]/10))\n",
    "            \n",
    "            dw = np.int64(np.min(np.diff(np.sort(np.unique(df_sub[\"X_coord\"]))))* 0.9) * pixel_factor\n",
    "            dh = np.int64(dw / im_w * im_h *plot_height/plot_width)\n",
    "            \n",
    "            \n",
    "            image = cv2.resize(image, (dw, dh),\n",
    "               interpolation = cv2.INTER_AREA)\n",
    "            image = np.flipud(np.ceil(image))\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            background = np.ones((pixel_x*pixel_factor,pixel_y*pixel_factor)) * 255 \n",
    "            \n",
    "            for h in range(len(df_sub)):\n",
    "                corner_x = np.int64(df_sub[\"X_coord\"].iloc[h]*pixel_factor) - x_offset *2\n",
    "                corner_y = np.int64(df_sub[\"Y_coord\"].iloc[h]*pixel_factor) - y_offset*2\n",
    "                \n",
    "\n",
    "                image_c = copy.deepcopy(image)\n",
    "                \n",
    "                image_c[image_c < 250] = df_sub[parameter].iloc[h]\n",
    "                image_c[image_c >= 250] = np.mean([upper,lower])\n",
    "                \n",
    "\n",
    "                background[corner_y:corner_y+np.shape(image_c)[0],corner_x:corner_x+np.shape(image_c)[1]] = image_c\n",
    "                \n",
    "            background[background >= 250] = np.mean([upper,lower])\n",
    "            \n",
    "            \n",
    "            p.image(image=[background],  dh=pixel_x, dw=pixel_y, x=0, y=0, color_mapper = mapper)\n",
    "#        \n",
    "        elif Visual == 'Surface':\n",
    "        \n",
    "            nx = 100\n",
    "            ny = 100\n",
    "\n",
    "            xi = np.linspace(df_sub[\"X_coord\"].min(), df_sub[\"X_coord\"].max(), nx)\n",
    "            yi = np.linspace(df_sub[\"Y_coord\"].min(), df_sub[\"Y_coord\"].max(), ny)\n",
    "            xi, yi = np.meshgrid(xi, yi)\n",
    "\n",
    "            zi = griddata( (df_sub[\"X_coord\"],df_sub[\"Y_coord\"]), df_sub[parameter], (xi,yi), method = 'linear' )\n",
    "\n",
    "\n",
    "            dh = 320\n",
    "            dw = 480\n",
    "\n",
    "            p.image(image=[zi], dh=dh, dw=dw, x=0, y=0, color_mapper = mapper) \n",
    "            \n",
    "        else: \n",
    "            if len(df_sub)>1:\n",
    "                width = np.int64(np.min(np.diff(np.sort(np.unique(df_sub[\"X_coord\"])))))\n",
    "                height = np.int64(np.min(np.diff(np.sort(np.unique(df_sub[\"Y_coord\"])))))\n",
    "            else:\n",
    "                width = 10\n",
    "                height = 10\n",
    "            \n",
    "            p.rect(\n",
    "                x=\"X_coord\",\n",
    "                y=\"Y_coord\",\n",
    "                width=width,\n",
    "                height=height,\n",
    "                source=source,\n",
    "                line_color='black',\n",
    "                line_width = 0.5,\n",
    "                fill_color=transform(parameter, mapper))\n",
    "\n",
    "        \n",
    "        p.text(x=\"X_coord\", y=\"Y_coord\",x_offset = -x_offset,y_offset = y_offset-2, text=parameter+'_mini',source=source,\n",
    "              text_font_size = {'value': '12px'},text_color='black')\n",
    "\n",
    "        Hover_p = [(\"Part number\", \"@Part_number\"),]\n",
    "        p.add_tools(HoverTool(tooltips=Hover_p))\n",
    "\n",
    "\n",
    "#         ticker = SingleIntervalTicker(interval=1, num_minor_ticks=0)\n",
    "#         xaxis = LinearAxis(ticker=ticker)\n",
    "#         yaxis = LinearAxis(ticker=ticker)\n",
    "#         p.add_layout(xaxis,\"below\")\n",
    "#         p.add_layout(yaxis,\"left\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Add all plots to parameter\n",
    "        plots.append(p)\n",
    "    return plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6b72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_formatter(my_col):\n",
    "    template = \"\"\"\n",
    "        <div style=\"background:<%= \n",
    "            (function colorfromint(){\n",
    "                if(result_col == true){\n",
    "                    return('#8a9f42')}\n",
    "                else if (result_col == false)\n",
    "                    {return('#f14e08')}\n",
    "                }()) %>; \n",
    "            color: black\"> \n",
    "        <%= value %>\n",
    "        </div>\n",
    "    \"\"\".replace('result_col',my_col)\n",
    "\n",
    "    return HTMLTemplateFormatter(template=template)\n",
    "\n",
    "\n",
    "\n",
    "#Create Homogeneity plots\n",
    "def homogeneity(data,parameters,Colourmap_range,Visual):\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    #If more layers, split based on layer            \n",
    "    if 'Print_ID_layer' in data.columns:\n",
    "        Jobs = np.unique(data['Print_ID_layer'])\n",
    "    else:\n",
    "        Jobs = np.unique(data['Print_ID'])\n",
    "\n",
    "    #Prepare collection parameters\n",
    "    Plots=[]\n",
    "    tabs1 = []\n",
    "    job = []\n",
    "    \n",
    "    #Convert matplotlib colorbar\n",
    "    m_coolwarm_rgb = (255 * cm.bwr(range(256))).astype('int')\n",
    "    coolwarm_palette = [RGB(*tuple(rgb)).to_hex() for rgb in m_coolwarm_rgb]\n",
    "\n",
    "\n",
    "    #Make mean heatmaps \n",
    "    plots = []\n",
    "    method = 'Mean'\n",
    "    plots_mean = heatmaps(data,parameters,job,Colourmap_range,method,coolwarm_palette,plots,Visual)\n",
    "\n",
    "    sec_plot_mean = column(*plots_mean)\n",
    "    \n",
    "\n",
    "    tab_mean = Panel(child=sec_plot_mean, title = method)\n",
    "    tabs1.append(tab_mean)\n",
    "    \n",
    "    \n",
    "    #Make std heatmaps\n",
    "    plots = []\n",
    "    method = 'Std'\n",
    "    plots_std = heatmaps(data,parameters,job,Colourmap_range,method,coolwarm_palette,plots,Visual)\n",
    "\n",
    "    sec_plot_std = column(*plots_std)\n",
    "\n",
    "\n",
    "    tab_std = Panel(child=sec_plot_std, title = method)\n",
    "    tabs1.append(tab_std)\n",
    "\n",
    "\n",
    "\n",
    "    #Make job based heatmaps \n",
    "    for job in Jobs:\n",
    "        plots = []\n",
    "\n",
    "\n",
    "        method = 'Original'\n",
    "        plots = heatmaps(data,parameters,job,Colourmap_range,method,coolwarm_palette,plots,Visual)\n",
    "        \n",
    "        #Layout \n",
    "        sec_plot = column(*plots)\n",
    "\n",
    "        tab = Panel(child=sec_plot, title = job)\n",
    "        tabs1.append(tab)\n",
    "\n",
    "\n",
    "    #Collect plots. Make two coloumns of tabs for easier comparison \n",
    "    Plot = Tabs(tabs=tabs1)\n",
    "    Plot_copy = Tabs(tabs=tabs1)\n",
    "    Plots = child=row(Plot, Plot_copy)\n",
    "    \n",
    "    \n",
    "    return Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9ef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aca2a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box and mean plot \n",
    "def Boxplot(Data,parameter,category):\n",
    "    # generate some synthetic time series for different categories\n",
    "    \n",
    "    \n",
    "    cat = Data[category]\n",
    "    cats = list(np.unique(cat))\n",
    "    ### ----- Use this if catagory is a date ----- ###\n",
    "    #cats = list(np.unique([cats.strftime(\"%m/%d\") for cats in cat]))\n",
    "\n",
    "    df_box = Data\n",
    "    df_box['score'] = Data[parameter]\n",
    "\n",
    "\n",
    "    # find the mean, quartiles and IQR for each category\n",
    "    groups = df_box[['score',category]].groupby(category)\n",
    "\n",
    "    q1 = groups.quantile(q=0.25)\n",
    "    q2 = groups.quantile(q=0.5)\n",
    "    q3 = groups.quantile(q=0.75)\n",
    "    iqr = q3 - q1\n",
    "    upper = q3 + 1.5*iqr\n",
    "    lower = q1 - 1.5*iqr\n",
    "    \n",
    "    mean = groups.mean(numeric_only=True)\n",
    "\n",
    "\n",
    "    # find the outliers for each category\n",
    "    def outliers(group):\n",
    "        cat = group.name\n",
    "        return group[(group.score > upper.loc[cat]['score']) | (group.score < lower.loc[cat]['score'])]['score']\n",
    "    out = groups.apply(outliers).dropna()\n",
    "    \n",
    "\n",
    "    # prepare outlier data for plotting, we need coordinates for every outlier.\n",
    "    \n",
    "    if len(cats)>1:\n",
    "        if not out.empty:\n",
    "            outx = list(out.index.get_level_values(0))\n",
    "            #outx = [outxx.strftime(\"%m/%d\") for outxx in outx]\n",
    "            outy = list(out.values)\n",
    "            outindex = list(out.index.get_level_values(1))\n",
    "            outpart = Data['Part_number'][outindex]\n",
    "                \n",
    "\n",
    "\n",
    "    # if no outliers, shrink lengths of stems to be no longer than the minimums or maximums\n",
    "    qmin = groups.quantile(q=0.00)\n",
    "    qmax = groups.quantile(q=1.00)\n",
    "    upper.score = [min([x,y]) for (x,y) in zip(list(qmax.loc[:,'score']),upper.score)]\n",
    "    lower.score = [max([x,y]) for (x,y) in zip(list(qmin.loc[:,'score']),lower.score)]\n",
    "\n",
    "    #Color for boxes\n",
    "    fill_color = []\n",
    "    for job in np.unique(cat):\n",
    "        mask = job == Data[category]\n",
    "        \n",
    "        fill_color.append('green')\n",
    "        \n",
    "        #If daraframe has info on whether part is accepted or not\n",
    "        #if np.sum(df_box['Accepted_'+code][mask]!='Yes')!=0:\n",
    "        #    fill_color.append('red')\n",
    "        #else:\n",
    "        #    fill_color.append('green')\n",
    "            \n",
    "    #Make data frame to enable hover tool    \n",
    "    vbar = np.ones(len(cats))*0.7\n",
    "    d = {\"cats\": cats, \"fill_color\": fill_color, \"q1score\": q1.score, \"q2score\": q2.score, \"q3score\": q3.score, \"upperscore\": upper.score, \"lowerscore\": lower.score, \"meanscore\": mean.score}\n",
    "    box_data = pd.DataFrame(d).reset_index() \n",
    "    box_data = ColumnDataSource(data=box_data)\n",
    "\n",
    "    #Make boxplot\n",
    "    p = figure(x_range=cats,width=800, height=350, x_axis_label = category, y_axis_label = 'Result value')\n",
    "\n",
    "    # stems\n",
    "    p.segment(\"cats\", \"upperscore\", \"cats\", \"q3score\", source = box_data, line_color=\"black\")\n",
    "    p.segment(\"cats\", \"lowerscore\", \"cats\", \"q1score\", source = box_data, line_color=\"black\")\n",
    "\n",
    "    # boxes\n",
    "    p.vbar(x=\"cats\", top=\"q3score\", bottom=\"q2score\", width=0.7, source = box_data, fill_color=\"fill_color\", line_color=\"black\")\n",
    "    p.vbar(x=\"cats\", top=\"q1score\", bottom=\"q2score\", width=0.7, source = box_data, fill_color=\"fill_color\", line_color=\"black\")\n",
    "\n",
    "    # whiskers (almost-0 height rects simpler than segments)\n",
    "    p.rect(\"cats\", \"lowerscore\", 0.2, 0.00001, source = box_data, line_color=\"black\")\n",
    "    p.rect(\"cats\", \"upperscore\", 0.2, 0.00001, source = box_data, line_color=\"black\")\n",
    "    \n",
    "    #Add mean and standard deveation from acceptn\n",
    "    #p.line([cats[0], cats[-1]], [Acceptance_values['Mean_'+code], Acceptance_values['Mean_'+code]], line_color = 'black')\n",
    "    #p.line([cats[0], cats[-1]], [Acceptance_values['Mean_'+code]+Acceptance_values['STD_'+code], Acceptance_values['Mean_'+code]+Acceptance_values['STD_'+code]], line_color = 'black',line_dash = 'dashed')\n",
    "    #p.line([cats[0], cats[-1]], [Acceptance_values['Mean_'+code]-Acceptance_values['STD_'+code], Acceptance_values['Mean_'+code]-Acceptance_values['STD_'+code]], line_color = 'black',line_dash = 'dashed')\n",
    " \n",
    "\n",
    "    # outliers\n",
    "    if len(cats)>1:\n",
    "        if not out.empty:\n",
    "\n",
    "            outdf = pd.DataFrame({\"outx\": outx,\"outy\": outy, \"outpart\": outpart}).reset_index() \n",
    "            p.scatter(\"outx\", \"outy\", source=outdf, size=16, color=\"darkorange\",fill_alpha=0.6)\n",
    "\n",
    "    #p.xaxis.major_label_text_font_size=\"16px\"\n",
    "    #p.yaxis.major_label_text_font_size=\"16px\"\n",
    "    \n",
    "\n",
    "    #Add hover tool to boxplot\n",
    "    Hover_p = [\n",
    "    (\"Median\", \"@q2score\"),\n",
    "    (\"Upper quantile\", \"@q3score\"),\n",
    "    (\"Upper quantile\", \"@q1score\"),\n",
    "    (\"Part number\", \"@outpart\"),\n",
    "    ]\n",
    "    p.add_tools(HoverTool(tooltips=Hover_p))\n",
    "    \n",
    "    p.add_layout(Title(text=Data['Version'].iloc[-1], align=\"right\",text_color ='grey'), \"right\")\n",
    "\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad8c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values to calculate CpK values\n",
    "c4=[0.7979,0.8862,0.9213,0.94,0.9515,0.9594,0.965,0.9693,0.9727,0.9754,0.9776,0.9794,0.981,0.9823,0.9835,0.9845,0.9854,0.9862,0.9869,0.9876,0.9882,0.9887,0.9892,0.9896]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e5e258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capability report \n",
    "def Capability(df,parameters,Tolerances,category):\n",
    "    \n",
    "\n",
    "    \n",
    "    Jobs = np.unique(df['Print_ID'])\n",
    "    \n",
    "    #df preparation \n",
    "    df_mean = pd.DataFrame({})\n",
    "    \n",
    "    tabs1 = []\n",
    "\n",
    "    data=df.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for parameter in parameters:\n",
    "        df_mean_mini = pd.DataFrame({})\n",
    "        \n",
    "        i=0\n",
    "        plots = []\n",
    "\n",
    "        colors = itertools.cycle(viridis(len(Jobs))) \n",
    "\n",
    "        #Calulate mean and std for full data set\n",
    "        Overall_mean = np.mean(data[parameter])\n",
    "        Overall_std = np.std(data[parameter])\n",
    "        \n",
    "        #Prep repeatability column\n",
    "        data[parameter+'_repeatability'] = -9999\n",
    "        \n",
    "        \n",
    "        for part in np.unique(data['Part_number']):\n",
    "            \n",
    "            part_mask = data['Part_number'] == part\n",
    "\n",
    "            \n",
    "            #Calculate repeatability as result value shifted by part mean\n",
    "            data[parameter+'_repeatability'][part_mask] = data[parameter][part_mask] - np.mean(data[parameter][part_mask])\n",
    "\n",
    "        # Repeatability based on all jobs\n",
    "        Repeatability = np.std(data[parameter+'_repeatability'])\n",
    "\n",
    "        #Plot of all jobs consecutively \n",
    "        p = figure(\n",
    "            plot_width=1000,\n",
    "            plot_height=350,\n",
    "            x_axis_label='Part Number + shift',\n",
    "            y_axis_label='Result_value',\n",
    "            title=parameter +'. Overall mean +/- std: '+ str(np.round(Overall_mean ,3)) +'+/-' + str(np.round(Overall_std,3))\n",
    "            )\n",
    "        p.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'),\"right\")\n",
    "        \n",
    "        #Part mean\n",
    "        ppp = figure(\n",
    "            plot_width=1000,\n",
    "            plot_height=350,\n",
    "            x_axis_label='Part Number',\n",
    "            y_axis_label='Mean result value',\n",
    "            title='Part mean'\n",
    "            )\n",
    "        ppp.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'),\"right\")\n",
    "\n",
    "        #Repeatability\n",
    "        pp = figure(\n",
    "            plot_width=1000,\n",
    "            plot_height=350,\n",
    "            x_axis_label='Part Number',\n",
    "            y_axis_label='Difference from part mean',\n",
    "            title='Repeatability: '+ str(np.round(Repeatability ,3))\n",
    "            )\n",
    "        pp.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'),\"right\")\n",
    "        \n",
    "        #Histogram\n",
    "        pppp = figure(\n",
    "            plot_width=1000,\n",
    "            plot_height=350,\n",
    "            x_axis_label='Result Value',\n",
    "            y_axis_label='Count (scaled)',\n",
    "            #title='Repeatability: '+ str(np.round(Repeatability ,3))\n",
    "            )\n",
    "        pppp.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'),\"right\")\n",
    "\n",
    "\n",
    "        for job in Jobs:\n",
    "            \n",
    "            color=next(colors)\n",
    "\n",
    "            #define subset based on job\n",
    "            df_sub = data[data['Print_ID']==job]\n",
    "            #Shift part number\n",
    "            df_sub['Part_number_scaled'] = df_sub['Part_number'] + np.max(df_sub['Part_number'])*i\n",
    "            i+=1\n",
    "\n",
    "\n",
    "            #Mean and std of job specific subset\n",
    "            mean = np.mean(df_sub[parameter])\n",
    "            std = np.std(df_sub[parameter])\n",
    "\n",
    "\n",
    "            df_sub[parameter+'_mean'] = mean\n",
    "            df_sub[parameter+'_std'] = std\n",
    "            df_sub[parameter+'_std_upper'] = mean+std\n",
    "            df_sub[parameter+'_std_lower'] = mean-std\n",
    "            \n",
    "\n",
    "\n",
    "            #Plot consecutive jobs\n",
    "            p.scatter('Part_number_scaled',parameter,color=color,source=df_sub,legend_label=job)            \n",
    "            p.line('Part_number_scaled',parameter+'_mean',source=df_sub,color='black')\n",
    "            p.line('Part_number_scaled',parameter+'_std_upper',source=df_sub,color='black',line_dash='dashed')\n",
    "            p.line('Part_number_scaled',parameter+'_std_lower',source=df_sub,color='black',line_dash='dashed')\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Add tolerance parameters if they exist \n",
    "            if parameter+'_USL' in Tolerances:\n",
    "                df_out = df_sub[(df_sub[parameter]>Tolerances[parameter+'_USL'].values[0]) | (df_sub[parameter]<Tolerances[parameter+'_LSL'].values[0])]\n",
    "                p.scatter('Part_number_scaled',parameter,color='red',source=df_out)\n",
    "                p.line( [np.min(df_sub['Part_number_scaled']),np.max(df_sub['Part_number_scaled'])], [Tolerances[parameter+'_USL'],Tolerances[parameter+'_USL']],color='gray'  )\n",
    "                p.line( [np.min(df_sub['Part_number_scaled']),np.max(df_sub['Part_number_scaled'])], [Tolerances[parameter+'_LSL'],Tolerances[parameter+'_LSL']],color='gray'  )\n",
    "\n",
    "                \n",
    "            # Add hover toll \n",
    "            parameter_name = '@'+ parameter\n",
    "            Hover_p = [(\"Part number\", \"@Part_number\"),(\"Print ID\", \"@Print_ID\"),(parameter,parameter_name)]\n",
    "            p.add_tools(HoverTool(tooltips=Hover_p))\n",
    "\n",
    "            #Add legend \n",
    "            p.legend.click_policy=\"hide\"\n",
    "            plots.append(p)\n",
    "            \n",
    "            \n",
    "            df_mini = df_sub.head(1)\n",
    "            \n",
    "         \n",
    "        #Prep for capability graphs \n",
    "        Category = np.unique(df[category])    \n",
    "        colors = itertools.cycle(viridis(len(Category))) \n",
    "        \n",
    "        df_control = pd.DataFrame({})\n",
    "        \n",
    "        \n",
    "        # Xbar chart for evolution of mean value\n",
    "        xbar_chart = figure(\n",
    "        plot_width=1000,\n",
    "        plot_height=350,\n",
    "        x_axis_label=category,\n",
    "        y_axis_label='Mean',\n",
    "        x_range=Category,\n",
    "        #title='Repeatability: '+ str(np.round(Repeatability ,3))\n",
    "        )\n",
    "        xbar_chart.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'),\"right\")\n",
    "        \n",
    "        # S chart for evolution of std value\n",
    "        s_chart = figure(\n",
    "        plot_width=1000,\n",
    "        plot_height=350,\n",
    "        x_axis_label=category,\n",
    "        y_axis_label='Std',\n",
    "        x_range=Category,\n",
    "        #title='Repeatability: '+ str(np.round(Repeatability ,3))\n",
    "        )\n",
    "        s_chart.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'),\"right\")\n",
    "        \n",
    "        x_range = Range1d(start = -1, end = 5)\n",
    "        y_range = Range1d(start = -2.2, end = 2.2)\n",
    "        cpk_plot = figure(\n",
    "            plot_width=400, \n",
    "            plot_height=300, \n",
    "            x_axis_type=None, \n",
    "            y_axis_type=None, \n",
    "            x_range=x_range, \n",
    "            y_range=y_range,\n",
    "            toolbar_location= None)\n",
    "        cpk_plot.add_layout(Title(text=data['Version'].iloc[-1], align=\"right\",text_color ='grey'), \"right\")\n",
    "        \n",
    "        for job in Category:\n",
    "            color=next(colors)\n",
    "            \n",
    "            df_sub2 = data[data[category]==job]\n",
    "            \n",
    "            \n",
    "            #Histograms\n",
    "            if not df_sub2[parameter].dropna().empty:\n",
    "                \n",
    "                hist, edges = np.histogram(df_sub2[parameter].dropna(), density=True, bins=15)\n",
    "    \n",
    "    \n",
    "                #Anderson-Darling test to check for normality at 95% confidence \n",
    "                result = (anderson(df_sub2[parameter].dropna(), dist='norm'))\n",
    "                AD_test = result.statistic < result.critical_values[2]\n",
    "\n",
    "                if np.isnan(result.statistic):\n",
    "                    AD_test = 'ERROR: Contains nan'\n",
    "                    line = '..'\n",
    "                else:\n",
    "                    if AD_test == True:\n",
    "                        AD_test = 'Normal'\n",
    "                        line = 'solid'\n",
    "                    else:\n",
    "                        AD_test = 'Not normal'\n",
    "                        line = 'dashed'\n",
    "            \n",
    "                # Calculate gaussian fit \n",
    "                x = np.linspace(np.min(df_sub2[parameter]), np.max(df_sub2[parameter]), 100)\n",
    "\n",
    "                mu = np.mean(df_sub2[parameter])\n",
    "                sigma = np.std(df_sub2[parameter])\n",
    "                \n",
    "                df_sub2[parameter+'_mean'] = mu\n",
    "                df_sub2[parameter+'_std'] = sigma\n",
    "                \n",
    "\n",
    "                pdf = 1/(sigma * np.sqrt(2*np.pi)) * np.exp(-(x-mu)**2 / (2*sigma**2))\n",
    "                \n",
    "                df_plot = pd.DataFrame({'x': x, 'pdf':pdf})\n",
    "\n",
    "                top = np.max(pdf)\n",
    "                \n",
    "                df_plot['Mean'] = mu\n",
    "                df_plot['Top'] = top\n",
    "                df_plot['Print_ID'] = job\n",
    "                df_plot['Std'] = std\n",
    "\n",
    "                \n",
    "                #Plot histogram \n",
    "                pppp_legend = job+', '+AD_test+'. Mean '+str(np.round(mu,3))+' +/- '+str(np.round(sigma,3))\n",
    "                pppp.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:],fill_color=color, line_color=\"white\",\n",
    "                          alpha=0.5,legend_label=pppp_legend)\n",
    "                pppp.line('x','pdf',line_dash=line,legend_label=pppp_legend,color=color,source=df_plot)\n",
    "\n",
    "\n",
    "                df_control = pd.concat([ df_control,pd.DataFrame({'Category': [job], 'Mean': [mu], 'Std' : [sigma]}) ])\n",
    "    \n",
    "                df_new = pd.DataFrame({'Category': [job], parameter: [str(np.round(mu,3))+'(std:'+str(np.round(sigma,3))+')']})\n",
    "                \n",
    "                if df_mean_mini.empty:\n",
    "                    df_mean_mini = df_new\n",
    "                else:\n",
    "                    df_mean_mini = pd.concat([ df_mean_mini,df_new])\n",
    "                    \n",
    "        df_new = pd.DataFrame({'Category': ['All prints'], parameter: [str(np.round(Overall_mean,3))+'(std:'+str(np.round(Overall_std,3))+')']})\n",
    "        if df_mean_mini.empty:\n",
    "            df_mean_mini = df_new\n",
    "        else:\n",
    "            df_mean_mini = pd.concat([ df_mean_mini,df_new])\n",
    "\n",
    "\n",
    "        if df_mean.empty:\n",
    "            df_mean = df_mean_mini \n",
    "        else:\n",
    "            df_mean = df_mean.merge(df_mean_mini, on = 'Category')\n",
    "            \n",
    "            \n",
    "        # Add legend \n",
    "        leg = p.legend[0]\n",
    "        p.add_layout(leg,'right')\n",
    "        \n",
    "        \n",
    "        # Add legend \n",
    "        leg = pppp.legend[0]\n",
    "        pppp.add_layout(leg,'right')\n",
    "        pppp.legend.click_policy=\"hide\"\n",
    "\n",
    "        \n",
    "        #Plot repetability data \n",
    "        pp.scatter('Part_number',parameter+'_repeatability',color='Peru',source=data)\n",
    "\n",
    "        #Add hover tool to repeatability plot\n",
    "        repeatability_name = '@'+parameter+'_repeatability'\n",
    "        Hover_pp = [(\"Part number\", \"@Part_number\"),('Print_ID','@Print_ID')\n",
    "                    ,(parameter,parameter_name),('Difference from mean',repeatability_name)]\n",
    "        pp.add_tools(HoverTool(tooltips=Hover_pp))\n",
    "        \n",
    "\n",
    "        #Plot part mean\n",
    "        IDs = data[['X_coord','Y_coord','Print_ID','Print_ID_layer','Print_ID_galvo']]\n",
    "        data_dropped = data.drop(columns=['X_coord','Y_coord','Print_ID','Print_ID_layer','Print_ID_galvo'])\n",
    "        df_part_mean = data_dropped.groupby('Part_number').mean(numeric_only=True)\n",
    "        df_part_std = data_dropped.groupby('Part_number').std()\n",
    "        \n",
    "        df_part_mean = df_part_mean.reset_index()\n",
    "        df_part_mean = df_part_mean.merge(IDs, left_index=True, right_index=True)\n",
    "        \n",
    "        df_part_std = df_part_std.reset_index()\n",
    "        df_part_std = df_part_std.merge(IDs, left_index=True, right_index=True)\n",
    "        \n",
    "        ppp.scatter('Part_number',parameter,color='Navy',source=df_part_mean)\n",
    "        \n",
    "        #If more than one job plot std as errorvbars on part mean value \n",
    "        for i in df_part_mean.index:\n",
    "\n",
    "            if (np.isnan(df_part_mean[parameter][i])==False) & (np.isnan(df_part_std[parameter][i])==False):\n",
    "                ppp.line([df_part_mean['Part_number'][i],df_part_mean['Part_number'][i]],\n",
    "                         [df_part_mean[parameter][i]-df_part_std[parameter][i],\n",
    "                          df_part_mean[parameter][i]+df_part_std[parameter][i]],\n",
    "                          color='black')\n",
    "\n",
    "        #Add hover tool \n",
    "        parameter_name = '@'+parameter\n",
    "        Hover_ppp = [(\"Part number\", \"@Part_number\"),(parameter,parameter_name)]\n",
    "        ppp.add_tools(HoverTool(tooltips=Hover_ppp))\n",
    "\n",
    "\n",
    "\n",
    "        #Box plot\n",
    "        boxplot = Boxplot(data,parameter,category)\n",
    "        \n",
    "        \n",
    "        #Control charts\n",
    "        df_control = df_control.reset_index(drop=True)\n",
    "        xbar_chart.line('Category','Mean',source=df_control,color='blue')\n",
    "        xbar_chart.scatter('Category','Mean',source=df_control,color='blue')\n",
    "        \n",
    "        Hover_xbar_chart = [(\"Category\", \"@Category\"),('Mean','@Mean')]\n",
    "        xbar_chart.add_tools(HoverTool(tooltips=Hover_xbar_chart))\n",
    "        \n",
    "        \n",
    "        s_chart.line('Category','Std',source=df_control,color='Orange')\n",
    "        s_chart.scatter('Category','Std',source=df_control,color='Orange')\n",
    "        \n",
    "        Hover_s_chart = [(\"Category\", \"@Category\"),('Std','@Std')]\n",
    "        s_chart.add_tools(HoverTool(tooltips=Hover_s_chart))\n",
    "        \n",
    "        \n",
    "        xcoor = [0,0,2,2,0,0,2,2]\n",
    "        ycoor = [1,0,1,0,-1,-2,-1,-2]\n",
    "        \n",
    "        #If tolerance is given add control chart calculations\n",
    "        if parameter+'_USL' in Tolerances:\n",
    "            \n",
    "            \n",
    "            pppp.line([Tolerances[parameter+'_LSL'],Tolerances[parameter+'_LSL']],[0,top],color='black')\n",
    "            pppp.line([Tolerances[parameter+'_USL'],Tolerances[parameter+'_USL']],[0,top],color='black')\n",
    "            \n",
    "            \n",
    "            xbar_chart.line([0,len(df_control['Category'])],[Tolerances[parameter+'_LSL'],Tolerances[parameter+'_LSL']],color='black')\n",
    "            xbar_chart.line([0,len(df_control['Category'])],[Tolerances[parameter+'_USL'],Tolerances[parameter+'_USL']],color='black')\n",
    "\n",
    "            \n",
    "            \n",
    "            #Calculate CpK, Cp, Pp, PpK \n",
    "            \n",
    "            # Scale values based on bias correction constart c4 value in accordance with \n",
    "            # https://blog.minitab.com/en/marilyn-wheatleys-blog/how-cpk-and-ppk-are-calculated2c-part-2\n",
    "            # and https://web.mit.edu/2.810/www/files/readings/ControlChartConstantsAndFormulae.pdf\n",
    "            c4_index = len(Category)-2\n",
    "            if len(Category) < 2:\n",
    "                c4_index = 0\n",
    "            elif (len(Category) >= 15) & len(Category) < 25:\n",
    "                c4_index = -2\n",
    "            elif len(Category) >= 25:\n",
    "                c4_index = -1\n",
    "            \n",
    "            c4_select = c4[c4_index]\n",
    "            \n",
    "            xbar = Overall_mean\n",
    "            sigma = Overall_std\n",
    "            usl = Tolerances[parameter+'_USL']\n",
    "            lsl = Tolerances[parameter+'_LSL']\n",
    "\n",
    "            sigma_hat = np.mean(df_control['Std'])/c4_select\n",
    "\n",
    "            pp_ = (usl-lsl) / (6 * sigma)\n",
    "            pp_s = (pp_[0].round(3))\n",
    "            ppk = np.min( [usl-xbar,xbar-lsl] ) /(3 * sigma)\n",
    "            ppk_s = (ppk.round(2))            \n",
    "\n",
    "            cp = (usl-lsl) / (6 * sigma_hat)\n",
    "            cp_s = (cp[0].round(3))\n",
    "            cpk = np.min( [usl-xbar,xbar-lsl] ) /(3 * sigma_hat)\n",
    "            cpk_s = (cpk.round(2))\n",
    "\n",
    "             \n",
    "            Name = ['Cp', cp_s, 'Cpk', cpk_s, 'Pp', pp_s, 'Ppk', ppk_s]\n",
    "        else:\n",
    "            Name = ['Cp', '', 'Cpk', '', 'Pp', '', 'Ppk', '']\n",
    "\n",
    "\n",
    "        d = {\"xcoor\": xcoor, \"ycoor\": ycoor, \"Name\": Name}\n",
    "        cpk_source = pd.DataFrame(d).reset_index() \n",
    "\n",
    "\n",
    "        cpk_plot.text(x=\"xcoor\", y=\"ycoor\", text=\"Name\", source=cpk_source, text_font_size = {'value': '35px'})\n",
    "        \n",
    "        \n",
    "\n",
    "        #Plot layouts\n",
    "        sec_plot = column([p,pp,ppp,boxplot,pppp,xbar_chart,s_chart,cpk_plot])\n",
    "\n",
    "        tab = Panel(child=sec_plot, title = parameter)\n",
    "        tabs1.append(tab)\n",
    "        \n",
    "    #Summary table. Transpose for easier read\n",
    "    df_mean_transposed = df_mean.T\n",
    "\n",
    "    df_mean_transposed.columns = df_mean_transposed.iloc[0]\n",
    "\n",
    "    df_mean_transposed = df_mean_transposed.drop(df_mean_transposed.index[0])\n",
    "    df_mean_transposed['Parameter'] = df_mean_transposed.index\n",
    "    \n",
    "    df_mean_transposed = df_mean_transposed.reset_index()\n",
    "    \n",
    "    df_mean_transposed =  df_mean_transposed.drop(columns=['index'])\n",
    "\n",
    "    \n",
    "    temp_cols=df_mean_transposed.columns.tolist()\n",
    "    new_cols=temp_cols[-1:] + temp_cols[:-1]\n",
    "    df_mean_transposed=df_mean_transposed[new_cols]\n",
    "    \n",
    "    #Plot table\n",
    "    Columns = [TableColumn(field=Ci, title=Ci) for Ci in df_mean_transposed.columns] # bokeh columns\n",
    "    data_table = DataTable(columns=Columns, source=ColumnDataSource(df_mean_transposed))\n",
    "    \n",
    "    \n",
    "    table_tab = Panel(child=data_table, title = 'Summary')\n",
    "    tabs1.insert(0,table_tab)\n",
    "\n",
    "    Plots = Tabs(tabs=tabs1)\n",
    "                \n",
    "    return Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfeacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run calculations and plots\n",
    "def Run(Path,Data_folder,Save_folder,Parameters = [],Visual='Rectangles',split=True,Tolerances=[],Colourmap_range=[],category = 'Print_ID',PartID_loc='First'):\n",
    "    \n",
    "    #Make file\n",
    "    df_full = make_file(Path,Data_folder,Save_folder,Version,PartID_loc)\n",
    "        \n",
    "\n",
    "    #Define parameters to plot (or the ones to not)\n",
    "    if len(Parameters) != 0:\n",
    "        parameters = Parameters\n",
    "    else:\n",
    "        parameters = df_full.columns.drop(['Part_number','Cube','Date','Print_ID','X_coord','Y_coord','Layer', 'Print_ID_layer','Print_ID_galvo','Machine','Version'])\n",
    "    \n",
    "    for cols in ['Galvo','Exposure_order','Z_coord']:\n",
    "        if cols in parameters:\n",
    "            parameters = parameters[parameters != cols]\n",
    "    \n",
    "\n",
    "    #Plot homogeneity \n",
    "    if split:\n",
    "        tabs = []\n",
    "        for machine in np.unique(df_full['Machine']):\n",
    "            \n",
    "            sec_plot = homogeneity(df_full[df_full['Machine']==machine],parameters,Colourmap_range,Visual)\n",
    "\n",
    "            tab = Panel(child=sec_plot, title = str(machine))\n",
    "            tabs.append(tab)\n",
    "        Plots = Tabs(tabs=tabs)\n",
    "\n",
    "    else:\n",
    "        Plots = homogeneity(df_full,parameters,Colourmap_range,Visual)\n",
    "\n",
    "    output_file(filename=Save_folder+\"/Heatmaps.html\", title=\"Heatmaps\")\n",
    "    show(Plots)\n",
    "    \n",
    "  \n",
    "    #plot Capability plots      \n",
    "    if split:\n",
    "        tabs = []\n",
    "        for machine in np.unique(df_full['Machine']):\n",
    "            sec_plot = Capability(df_full[df_full['Machine']==machine],parameters,Tolerances,category)\n",
    "\n",
    "            tab = Panel(child=sec_plot, title = str(machine))\n",
    "            tabs.append(tab)\n",
    "        Plots = Tabs(tabs=tabs)\n",
    "    else:\n",
    "        Plots = Capability(df_full,parameters,Tolerances,category)    \n",
    "    \n",
    "        \n",
    "        \n",
    "    output_file(filename=Save_folder+\"/Capability.html\", title=\"Capability\")\n",
    "    show(Plots)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db9ebc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fdcd679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33495f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
